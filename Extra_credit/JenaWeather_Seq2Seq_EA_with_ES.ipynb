{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638b4e9b",
   "metadata": {},
   "source": [
    "# Multi-step Seq2Seq Time-Series Forecasting using Dual Evolutionary Algorithms on Jena Weather Dataset\n",
    "\n",
    "**Author:** V Harsha Yellela  \n",
    "**LTU ID:** 000798754  \n",
    "**Course:** MCS-5993 Evolutionary Computation & Deep Learning  \n",
    "**Project Type:** Extra Credit Project  \n",
    "**AI Assistance:** ~25% (used for structure, EA integration ideas; all code was understood and adapted by me)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook I am experimenting with **sequence-to-sequence (Seq2Seq)** models and **evolutionary strategies** for time-series forecasting.\n",
    "\n",
    "I use the **Jena Weather Dataset** and try to:\n",
    "\n",
    "1. Use past weather data (72 hours) to predict the next 6 hours (multi-step horizon).  \n",
    "2. Build an **encoder–decoder LSTM** model (Seq2Seq).  \n",
    "3. Optimize the model in **two ways**:\n",
    "   - A **Genetic Algorithm (GA)** to search for good hyperparameters (LSTM units, learning rate, dropout).\n",
    "   - An **ES(1+1) with 1/5 success rule** to fine-tune the final layer weights after training.\n",
    "\n",
    "Since I am still learning deep learning and evolutionary algorithms, I focus on writing clear steps and comments so I can come back later and remember what I did and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd623b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Configure Environment\n",
    "\n",
    "In this section I import the libraries, set random seeds, and configure the GPU (if available). I also enable mixed precision if supported, since this can speed up training on modern GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "# GPU configuration: allow memory growth and detect GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(f\"GPUs detected: {len(gpus)} - memory growth enabled.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "\n",
    "# Mixed precision for faster GPU training (if supported)\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled (float16 compute, float32 variables).\")\n",
    "except Exception as e:\n",
    "    print(\"Mixed precision not enabled:\", e)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0ec56",
   "metadata": {},
   "source": [
    "## 2. Load & Preprocess the Jena Weather Dataset\n",
    "\n",
    "Here I:\n",
    "\n",
    "- Load the CSV file.\n",
    "- Clean missing values (if any).\n",
    "- Select a small set of core weather features.\n",
    "- Standardize the features so the neural network trains more smoothly.\n",
    "\n",
    "> **Note:** I assume the file is called `WeatherJena.csv` and is located in the same folder as this notebook. If not, I can update the `csv_path` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Jena weather CSV file\n",
    "# Update this path if your file is stored somewhere else.\n",
    "csv_path = \"WeatherJena.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning: replace -200 with NaN (if present) and drop NaNs\n",
    "df = df.replace(-200, np.nan).dropna()\n",
    "print(\"After dropping NaNs:\", df.shape)\n",
    "\n",
    "# Select a subset of relevant features\n",
    "features = [\"T (degC)\", \"p (mbar)\", \"rho (g/m**3)\", \"wv (m/s)\", \"max. wv (m/s)\"]\n",
    "data = df[features].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Data scaled shape:\", data_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd566e34",
   "metadata": {},
   "source": [
    "## 3. Create Lagged Sequences (Windowing)\n",
    "\n",
    "For Seq2Seq forecasting I need:\n",
    "\n",
    "- **Input sequence:** past 72 hours (3 days) of weather data.  \n",
    "- **Output sequence:** next 6 hours of weather.\n",
    "\n",
    "I slide a window over the time-series and build `(X, y)` pairs:\n",
    "\n",
    "- `X[i]` = 72×features  \n",
    "- `y[i]` = 6×features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10026011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define past and future window sizes\n",
    "past_steps = 24 * 3   # 72 time steps (3 days of hourly data)\n",
    "future_steps = 6      # Predict next 6 hours\n",
    "\n",
    "def create_seq2seq_data(dataset, past_steps, future_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - past_steps - future_steps):\n",
    "        X.append(dataset[i:i+past_steps])\n",
    "        y.append(dataset[i+past_steps:i+past_steps+future_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_seq2seq_data(data_scaled, past_steps, future_steps)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Split into Train / Validation / Test sets (60% / 20% / 20%)\n",
    "n = len(X)\n",
    "train_end = int(0.6 * n)\n",
    "val_end = int(0.8 * n)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_end], X[train_end:val_end], X[val_end:]\n",
    "y_train, y_val, y_test = y[:train_end], y[train_end:val_end], y[val_end:]\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a4efc",
   "metadata": {},
   "source": [
    "## 4. Build Seq2Seq LSTM Model\n",
    "\n",
    "I use a simple encoder–decoder LSTM:\n",
    "\n",
    "- **Encoder:** reads the past 72 time steps and compresses them into hidden states.\n",
    "- **Decoder:** starts from the encoder states and predicts 6 future steps.\n",
    "- **TimeDistributed Dense:** maps each decoder output to the 5 weather features.\n",
    "\n",
    "This matches the Seq2Seq idea from class (time-series → time-series).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq(n_features, n_past, n_future, n_units=64, dropout_rate=0.2):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(n_past, n_features))\n",
    "    encoder_lstm = LSTM(n_units, activation='tanh', return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = RepeatVector(n_future)(encoder_outputs)\n",
    "    decoder_lstm = LSTM(n_units, activation='tanh', return_sequences=True)(\n",
    "        decoder_inputs, initial_state=encoder_states\n",
    "    )\n",
    "\n",
    "    # Optional dropout on decoder outputs\n",
    "    if dropout_rate and dropout_rate > 0:\n",
    "        decoder_lstm = Dropout(dropout_rate)(decoder_lstm)\n",
    "\n",
    "    # TimeDistributed Dense to generate multi-step, multi-feature outputs\n",
    "    # Force final predictions to float32 so mixed precision does not confuse metrics later\n",
    "    decoder_outputs = TimeDistributed(Dense(n_features, dtype='float32'))(decoder_lstm)\n",
    "\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Quick check\n",
    "n_features = X.shape[2]\n",
    "base_model = build_seq2seq(n_features=n_features, n_past=past_steps, n_future=future_steps)\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d2c25d",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm (GA) for Hyperparameter Search\n",
    "\n",
    "I use a **Genetic Algorithm** as an outer-loop optimizer to search over:\n",
    "\n",
    "- Number of LSTM units (32–128)\n",
    "- Learning rate (0.0005–0.01)\n",
    "- Dropout rate (0.1–0.5)\n",
    "\n",
    "To keep the search fast enough, I:\n",
    "\n",
    "- Use only a small subset of the training data for fitness evaluation.\n",
    "- Train each candidate for a few epochs.\n",
    "- Use selection, crossover, and mutation to evolve the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdaf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_optimize(pop_size=4, generations=3, subset_ratio=0.08, epochs_per_eval=2):\n",
    "    \"\"\"Genetic Algorithm for hyperparameter optimization.\"\"\"\n",
    "    # Choose a subset of training data for speed\n",
    "    subset_size = max(2048, int(len(X_train) * subset_ratio))\n",
    "    X_train_subset = X_train[:subset_size]\n",
    "    y_train_subset = y_train[:subset_size]\n",
    "\n",
    "    print(f\"Using {subset_size}/{len(X_train)} samples for GA fitness evaluation\")\n",
    "    print(f\"Subset ratio ≈ {100*subset_ratio:.1f}% of training data\\n\")\n",
    "\n",
    "    # Initialize population\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        # [units, lr, dropout]\n",
    "        ind = [\n",
    "            random.randint(32, 128),        # LSTM units\n",
    "            random.uniform(0.0005, 0.01),   # Learning rate\n",
    "            random.uniform(0.1, 0.5)        # Dropout rate\n",
    "        ]\n",
    "        population.append(ind)\n",
    "\n",
    "    best_losses_over_gens = []\n",
    "\n",
    "    def fitness(ind):\n",
    "        \"\"\"Train a small model and return final training loss as fitness.\"\"\"\n",
    "        units, lr, dropout = ind\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = build_seq2seq(\n",
    "            n_features=X.shape[2],\n",
    "            n_past=past_steps,\n",
    "            n_future=future_steps,\n",
    "            n_units=int(units),\n",
    "            dropout_rate=dropout\n",
    "        )\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "        subset_batch = 512\n",
    "        ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_train_subset.astype('float32'), y_train_subset.astype('float32'))\n",
    "        )\n",
    "        ds = ds.batch(subset_batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        history = model.fit(ds, epochs=epochs_per_eval, verbose=0)\n",
    "        return history.history['loss'][-1]\n",
    "\n",
    "    for gen in range(generations):\n",
    "        print(f\"Generation {gen+1}/{generations}:\")\n",
    "        scores = []\n",
    "        for i, ind in enumerate(population):\n",
    "            score = fitness(ind)\n",
    "            scores.append(score)\n",
    "            print(f\"  Individual {i+1}: Loss={score:.4f}, Units={int(ind[0])}, LR={ind[1]:.6f}, Dropout={ind[2]:.3f}\")\n",
    "\n",
    "        num_parents = max(2, pop_size // 2)\n",
    "        ranked = [x for _, x in sorted(zip(scores, population), key=lambda pair: pair[0])]\n",
    "        population = ranked[:num_parents]\n",
    "\n",
    "        best_loss = min(scores)\n",
    "        best_losses_over_gens.append(best_loss)\n",
    "        print(f\"  → Best Loss this generation: {best_loss:.4f}\\n\")\n",
    "\n",
    "        while len(population) < pop_size:\n",
    "            p1, p2 = random.sample(population, 2)\n",
    "            child = [(p1[i] + p2[i]) / 2 for i in range(len(p1))]\n",
    "            if random.random() < 0.3:\n",
    "                gene_idx = random.randint(0, 2)\n",
    "                child[gene_idx] *= random.uniform(0.8, 1.2)\n",
    "            population.append(child)\n",
    "\n",
    "    best_ind = population[0]\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "    print(f\"  Units: {int(best_ind[0])}\")\n",
    "    print(f\"  Learning Rate: {best_ind[1]:.6f}\")\n",
    "    print(f\"  Dropout Rate: {best_ind[2]:.3f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return best_ind, best_losses_over_gens\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING GENETIC ALGORITHM HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_hparams, ga_history = GA_optimize(\n",
    "    pop_size=4,\n",
    "    generations=3,\n",
    "    subset_ratio=0.08,\n",
    "    epochs_per_eval=2\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Best Hyperparameters:\")\n",
    "print(f\"  Units: {int(best_hparams[0])}\")\n",
    "print(f\"  Learning Rate: {best_hparams[1]:.6f}\")\n",
    "print(f\"  Dropout Rate: {best_hparams[2]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36189e26",
   "metadata": {},
   "source": [
    "## 6. Train Final Seq2Seq Model with Evolved Hyperparameters\n",
    "\n",
    "Now I take the best hyperparameters from the GA and train a full Seq2Seq model on the training set, with validation on a held-out validation set. I also use EarlyStopping and ReduceLROnPlateau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db262d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "units, lr, dropout = best_hparams\n",
    "\n",
    "final_model = build_seq2seq(\n",
    "    n_features=X.shape[2],\n",
    "    n_past=past_steps,\n",
    "    n_future=future_steps,\n",
    "    n_units=int(units),\n",
    "    dropout_rate=dropout\n",
    ")\n",
    "final_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=lr),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING FINAL MODEL WITH EVOLVED HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration: Units={int(units)}, LR={lr:.6f}, Dropout={dropout:.3f}\\n\")\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    batch_size = 512\n",
    "    print(f\"GPU detected - using batch size: {batch_size}\")\n",
    "else:\n",
    "    batch_size = 32\n",
    "    print(f\"CPU mode - using batch size: {batch_size}\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train.astype('float32'), y_train.astype('float32'))\n",
    ").shuffle(2048).batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_val.astype('float32'), y_val.astype('float32'))\n",
    ").batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "MAX_EPOCHS = 30\n",
    "print(f\"Starting training for up to {MAX_EPOCHS} epochs...\\n\")\n",
    "\n",
    "history = final_model.fit(\n",
    "    train_ds,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7954e",
   "metadata": {},
   "source": [
    "## 7. ES(1+1) with 1/5 Success Rule on Final Layer Weights\n",
    "\n",
    "Now I apply ES(1+1) on the final TimeDistributed Dense layer weights using a small validation subset as the objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES11_5_rule(obj_func, xp, step_size=0.05, window=20, max_gen=60):\n",
    "    best_val = obj_func(xp)\n",
    "    success_cnt = 0\n",
    "    loss_history = [best_val]\n",
    "    step_history = [step_size]\n",
    "\n",
    "    for g in range(1, max_gen + 1):\n",
    "        xo = xp + np.random.normal(0, step_size, size=xp.shape)\n",
    "        val = obj_func(xo)\n",
    "\n",
    "        if val < best_val:\n",
    "            xp, best_val = xo, val\n",
    "            success_cnt += 1\n",
    "\n",
    "        loss_history.append(best_val)\n",
    "        step_history.append(step_size)\n",
    "\n",
    "        if g % window == 0:\n",
    "            success_rate = success_cnt / window\n",
    "            if success_rate > 0.2:\n",
    "                step_size /= 0.82\n",
    "            elif success_rate < 0.2:\n",
    "                step_size *= 0.82\n",
    "            success_cnt = 0\n",
    "\n",
    "    return xp, best_val, loss_history, step_history\n",
    "\n",
    "# Validation subset for ES\n",
    "es_subset_size = min(1024, len(X_val))\n",
    "X_es = X_val[:es_subset_size].astype('float32')\n",
    "y_es = y_val[:es_subset_size].astype('float32')\n",
    "\n",
    "# Final dense (TimeDistributed inner layer)\n",
    "td_layer = final_model.layers[-1]\n",
    "dense_layer = td_layer.layer\n",
    "W, b = dense_layer.get_weights()\n",
    "W_shape = W.shape\n",
    "b_shape = b.shape\n",
    "xp_init = np.concatenate([W.flatten(), b.flatten()])\n",
    "\n",
    "def set_dense_weights_from_vector(vec):\n",
    "    W_size = np.prod(W_shape)\n",
    "    W_flat = vec[:W_size]\n",
    "    b_flat = vec[W_size:]\n",
    "    W_new = W_flat.reshape(W_shape)\n",
    "    b_new = b_flat.reshape(b_shape)\n",
    "    dense_layer.set_weights([W_new, b_new])\n",
    "\n",
    "def es_objective(vec):\n",
    "    set_dense_weights_from_vector(vec)\n",
    "    loss = final_model.evaluate(X_es, y_es, verbose=0)[0]\n",
    "    return loss\n",
    "\n",
    "print(\"Starting ES(1+1) optimization on final Dense layer weights...\")\n",
    "start_time = time.time()\n",
    "best_vec, best_val, es_loss_history, es_step_history = ES11_5_rule(\n",
    "    obj_func=es_objective,\n",
    "    xp=xp_init,\n",
    "    step_size=0.05,\n",
    "    window=20,\n",
    "    max_gen=60\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"ES(1+1) finished. Best validation subset loss: {best_val:.4f}\")\n",
    "print(f\"Time taken for ES optimization: {elapsed:.1f} seconds\")\n",
    "\n",
    "set_dense_weights_from_vector(best_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c79fba",
   "metadata": {},
   "source": [
    "## 8. Evaluation, Visualizations, and Metrics\n",
    "\n",
    "Here I look at training curves, GA and ES behaviour, and final forecast quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc652b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# GA best loss per generation\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(ga_history, marker='o')\n",
    "plt.title('GA: Best Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Best Training Loss (subset)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ES loss and step size curves\n",
    "gens = np.arange(len(es_loss_history))\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Generation')\n",
    "ax1.set_ylabel('Validation Subset Loss', color=color)\n",
    "ax1.plot(gens, es_loss_history, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Step Size', color=color)\n",
    "ax2.plot(gens, es_step_history, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('ES(1+1): Loss and Step Size over Generations')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c44d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and visualize predictions\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    batch_size = 512\n",
    "else:\n",
    "    batch_size = 32\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test.astype('float32'), y_test.astype('float32'))\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "y_pred = final_model.predict(test_ds)\n",
    "\n",
    "# Inverse scaling\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, X.shape[2]))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, X.shape[2]))\n",
    "y_test_inv = y_test_inv.reshape(y_test.shape)\n",
    "y_pred_inv = y_pred_inv.reshape(y_pred.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sample_size = min(200, len(y_test_inv))\n",
    "\n",
    "# Temp first step\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_test_inv[:sample_size, 0, 0], label='Actual Temp', alpha=0.7)\n",
    "plt.plot(y_pred_inv[:sample_size, 0, 0], label='Predicted Temp', alpha=0.7)\n",
    "plt.title('Temperature Forecast vs Actual (1st of 6 hours)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "\n",
    "# Pressure first step\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(y_test_inv[:sample_size, 0, 1], label='Actual Pressure', alpha=0.7)\n",
    "plt.plot(y_pred_inv[:sample_size, 0, 1], label='Predicted Pressure', alpha=0.7)\n",
    "plt.title('Pressure Forecast vs Actual (1st of 6 hours)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Pressure (mbar)')\n",
    "plt.legend()\n",
    "\n",
    "# Multi-step temp sequence for one sample\n",
    "plt.subplot(2, 2, 3)\n",
    "seq_idx = min(50, len(y_test_inv) - 1)\n",
    "time_steps = np.arange(future_steps)\n",
    "plt.plot(time_steps, y_test_inv[seq_idx, :, 0], 'o-', label='Actual Temp', markersize=4)\n",
    "plt.plot(time_steps, y_pred_inv[seq_idx, :, 0], 'o-', label='Predicted Temp', markersize=4)\n",
    "plt.title('6-Hour Temperature Sequence Prediction')\n",
    "plt.xlabel('Hour Ahead')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "\n",
    "# Correlation actual vs predicted (first step)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_test_inv[:, 0, 0], y_pred_inv[:, 0, 0], alpha=0.5)\n",
    "min_val = min(y_test_inv[:, 0, 0].min(), y_pred_inv[:, 0, 0].min())\n",
    "max_val = max(y_test_inv[:, 0, 0].max(), y_pred_inv[:, 0, 0].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Temperature (1st step)')\n",
    "plt.xlabel('Actual Temp (°C)')\n",
    "plt.ylabel('Predicted Temp (°C)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PERFORMANCE METRICS (AFTER GA + ES)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_names = [\"Temperature\", \"Pressure\", \"Density\", \"Wind Speed\", \"Max Wind Speed\"]\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    y_true = y_test_inv[:, :, i].flatten()\n",
    "    y_pred_flat = y_pred_inv[:, :, i].flatten()\n",
    "    mae = mean_absolute_error(y_true, y_pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred_flat))\n",
    "    r2 = r2_score(y_true, y_pred_flat)\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  MAE:  {mae:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  R²:   {r2:.3f}\\n\")\n",
    "\n",
    "all_true = y_test_inv.flatten()\n",
    "all_pred = y_pred_inv.flatten()\n",
    "overall_mae = mean_absolute_error(all_true, all_pred)\n",
    "overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "overall_r2 = r2_score(all_true, all_pred)\n",
    "\n",
    "print(\"OVERALL:\")\n",
    "print(f\"  MAE:  {overall_mae:.3f}\")\n",
    "print(f\"  RMSE: {overall_rmse:.3f}\")\n",
    "print(f\"  R²:   {overall_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63091a03",
   "metadata": {},
   "source": [
    "## 9. Reflection and Discussion (Student Perspective)\n",
    "\n",
    "In this project I combined what I learned about **time-series**, **Seq2Seq LSTMs**, and **evolutionary algorithms** into a single experiment.\n",
    "\n",
    "- I used 72 hours of past weather data to predict the next 6 hours.\n",
    "- I used a GA to automatically search for good hyperparameters.\n",
    "- I used ES(1+1) with the 1/5 success rule to fine-tune the last layer weights.\n",
    "\n",
    "This helped me see how evolutionary methods can sit on top of neural networks (for hyperparameters) and also inside them (for weights), and how the ideas from the lectures actually look in code and in plots.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
